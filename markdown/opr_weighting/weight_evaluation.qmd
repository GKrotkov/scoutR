---
title: "OPR Weighting Evaluation"
format: html
---

```{r setup, message = FALSE, warning = FALSE}
library(devtools)
load_all()
rm(list = ls())
load("../../data/district_quals_09_24.rda")

# normalize match scores so we can do cross-week and cross-season comparisons
# this simplifies a lot of later code
matches <- lapply(
    matches, 
    function(match_df){
        scaled <- scale(c(match_df$red_score, match_df$blue_score))
        match_df$red_score <- scaled[1:nrow(match_df)]
        match_df$blue_score <- scaled[(nrow(match_df) + 1):length(scaled)]
        return(match_df)
    })

library(extrafont)
library(gt)
gos_blue <- "#337DFC"
gos_red <- "#F7041A"

gos_theme <- theme_bw() + 
    theme(text = element_text(family = "Futura"))

load("../../data/opr_weights.rda")

weighted_fit <- function(match_df, w){
    return(
        fit_lineup_lm(
            match_df, 
            responses = list(
                red = match_df$red_score, blue = match_df$blue_score
            ), 
            w = w
        )
    )
}
```

# Test MSE

Reasons to reserve the last 3/12 matches instead of randomly selecting: 
- Casts the problem as a prediction, rather than inference, problem: we are interested in approximating a team's performance heading into the playoff rounds 
- Follows the sequential nature of the data 
- Treats every event in the same manner, while if we randomly picked there would be an interaction with the year effect which would be difficult to account for

```{r optimization-helpers}
# redefine compute_mse_diff to use WLS
# assume matches is a dataframe of matches with red_score and blue_score columns
compute_mse_diff <- function(matches, w){
    # reserve the last 3/12 matches for testing
    test <- weight_rows(matches, c(rep(0, 3), 1))
    train <- weight_rows(matches, c(rep(1, 3), 0))

    null_fit <- fit_lineup_lm(
        train, list(red = train$red_score, blue = train$blue_score)
    )

    weighted_fit <- fit_lineup_lm(
        train, list(red = train$red_score, blue = train$blue_score), w = w
    )

    test_design <- lineup_design_matrix(test)
    test_design$response <- c(test$blue_score, test$red_score)
    
    preds_null <- predict(null_fit, newdata = test_design)
    preds_weighted <- predict(weighted_fit, newdata = test_design)
    true_response <- c(test$blue_score, test$red_score)
    
    mse_null <- mean((preds_null - true_response) ^ 2)
    mse_weighted <- mean((preds_weighted - true_response) ^ 2)
    return(mse_null / mse_weighted)
}

# matches - list of dataframes, each with qual matches with >= 12 matches/team
# w - vector representing the weighting
# returns a vector of mean squared errors for each event in `matches`, where
# negative means that the weighted models performed better and positive means
# that the null models performed better.
test_weighting <- function(matches, w){
    # don't need to normalize weights anymore because the helper function does
    diffs <- sapply(
        matches, compute_mse_diff, w = w
    )
    names(diffs) <- names(matches)
    return(diffs)
}
```

```{r visualization}
mse_diffs <- test_weighting(matches, w_linear)

viz <- data.frame(mse_diff = mse_diffs, 
                  year = as.factor(substr(names(mse_diffs), 1, 4)))

ggplot(viz, aes(x = mse_diff)) + 
    geom_histogram(fill = gos_blue, color = "black", 
                   bins = ceiling(sqrt(nrow(viz)))) + 
    geom_vline(aes(xintercept = mean(mse_diff)), color = gos_red) + 
    annotate("label", x = mean(mse_diffs), y = 15, 
             label = paste0("Mean: ", round(mean(mse_diffs), 3))) +
    labs(title = "OPR Weighting Test MSE Ratio", 
         subtitle = "District Event Qualifications, 2009 - 2024", 
         x = "Event-Normalized MSE Ratio (null:weighted)", 
         y = "# Events") +
    gos_theme

ggplot(viz, aes(x = mse_diff)) + 
    geom_density(alpha = 0.2, fill = gos_blue) + 
    facet_wrap(~year) +
    labs(title = "OPR Weighting Test MSE Ratio", x = "Test MSE Ratio", 
         y = "Density") + 
    gos_theme

viz <- viz %>%
    group_by(year) %>%
    summarize(events = n(), 
              mean_diff = mean(mse_diff), 
              median_diff = median(mse_diff))

colnames(viz) <- c(
    "Year", "# Events", "Mean MSE Ratio", "Median MSE Ratio"
)

gt(viz) %>%
    tab_header(
        title = "Yearly MSE ratios raw:weighted OPR"
    ) %>%
    tab_options(
        column_labels.background.color = gos_blue
    ) %>%
    tab_style(
        cell_borders(color = gos_blue), 
        cells_body()
    ) %>%
    opt_table_font(
        font = "Futura"
    )
```

# In-Sample MSE

```{r}
compute_in_sample_mse <- function(matches, w){
    fits <- lapply(matches, weighted_fit, w = w)
    mses <- sapply(
        fits, function(fit){
           return(mean(residuals(fit) ^ 2))
        })
    return(mses)
}

weighted_mse <- compute_in_sample_mse(matches, w_linear)
raw_mse <- compute_in_sample_mse(matches, 1)

viz <- data.frame(mse_diff = raw_mse - weighted_mse, 
                  year = as.factor(substr(names(weighted_mse), 1, 4)))

ggplot(viz, aes(x = mse_diff)) + 
    geom_histogram(fill = gos_blue, color = "black", 
                   bins = ceiling(sqrt(nrow(viz)))) + 
    geom_vline(aes(xintercept = mean(mse_diff)), color = gos_red) + 
    annotate("label", x = mean(viz$mse_diff), y = 15, 
             label = paste0("Mean: ", round(mean(viz$mse_diff), 6))) +
    labs(title = "Weighting Improves OPR Predictions", 
         subtitle = "District Event Qualifications, 2009 - 2024", 
         x = "Event-Normalized Mean Squared Error Difference (Std Devs)", 
         y = "# Events") +
    gos_theme

ggplot(viz, aes(x = mse_diff)) + 
    geom_density(alpha = 0.2, fill = gos_blue) + 
    facet_wrap(~year) +
    labs(title = "OPR Weighting MSE Difference", x = "MSE Difference", 
         y = "Density") + 
    gos_theme

viz <- viz %>%
    group_by(year) %>%
    summarize(events = n(), 
              mean_diff = mean(mse_diff), 
              median_diff = median(mse_diff))

colnames(viz) <- c(
    "Year", "# Events", "Mean MSE Difference", "Median MSE Difference"
)

gt(viz) %>%
    tab_header(
        title = "Yearly MSE differences between null and weighted OPR"
    ) %>%
    tab_options(
        column_labels.background.color = gos_blue
    ) %>%
    tab_style(
        cell_borders(color = gos_blue), 
        cells_body()
    ) %>%
    opt_table_font(
        font = "Futura"
    )
```

# LOOCV

Advantages of LOOCV: 
- Retains more of the training test so the weighting makes more sense
- Unbiased estimator of prediction risk, low variance because of the averaging over the entire hat matrix.
- Shortcut formula can make it computationally efficient
    - Makes weight optimization over a grid possible

```{r}
# this uses the "shortcut" formula for LOOCV
# see formula 5.2 in ISLR by James, Witten, Hastie, and Tibshirani (v2), pg 202
LOOCV <- function(fit){
    mean(((fit$model$response - predict(fit))/(1 - boot::glm.diag(fit)$h))^2)
}

linear_weight_fits <- lapply(matches, weighted_fit, w = w_linear)
bin_weight_fits <- lapply(matches, weighted_fit, w = w_bin_resid_vars)
fitted_var_weight_fits <- lapply(matches, weighted_fit, w = w_bin_fitted_vars)
raw_fits <- lapply(matches, weighted_fit, w = 1)

linear_weight_loocvs <- sapply(linear_weight_fits, LOOCV)
bin_weight_loocvs <- sapply(bin_weight_fits, LOOCV)
fitted_weight_loocvs <- sapply(fitted_var_weight_fits, LOOCV)
raw_loocvs <- sapply(raw_fits, LOOCV)

viz <- data.frame(linear_loocv_ratio = raw_loocvs / linear_weight_loocvs, 
                  bin_loocv_ratio = raw_loocvs / bin_weight_loocvs,
                  year = as.factor(substr(names(raw_loocvs), 1, 4)))

ggplot(viz, aes(x = bin_loocv_ratio)) + 
    geom_histogram(fill = gos_blue, color = "black", 
                   bins = ceiling(sqrt(nrow(viz)))) + 
    geom_vline(aes(xintercept = mean(bin_loocv_ratio)), color = gos_red) + 
    annotate("label", x = mean(viz$bin_loocv_ratio), y = 15, 
             label = paste0("Mean: ", 
                            round(mean(viz$bin_loocv_ratio), 3))) +
    labs(title = "OPR Weighting LOOCV Ratio", 
         subtitle = "District Event Qualifications, 2009 - 2024", 
         x = "Event-Normalized LOOCV Error Ratio (raw:weighted)", 
         y = "# Events") +
    gos_theme

ggplot(viz, aes(x = bin_loocv_ratio)) + 
    geom_density(alpha = 0.2, fill = gos_blue) + 
    facet_wrap(~year) +
    labs(title = "OPR Weighting LOOCV Ratio", x = "LOOCV Difference", 
         y = "Density") + 
    gos_theme

viz <- viz %>%
    group_by(year) %>%
    summarize(events = n(), 
              linear_mean_ratio = mean(linear_loocv_ratio), 
              linear_median_ratio = median(linear_loocv_ratio), 
              bin_mean_ratio = mean(bin_loocv_ratio), 
              bin_median_ratio = median(bin_loocv_ratio))

colnames(viz) <- c(
    "Year", "# Events", 
    "Mean Linear LOOCV Ratio", "Median Linear LOOCV Ratio", 
    "Mean bin LOOCV Ratio", "Median bin LOOCV Ratio"
)

gt(viz) %>%
    tab_header(
        title = "LOOCV Ratios between weighted and raw OPR"
    ) %>%
    tab_options(
        column_labels.background.color = gos_blue
    ) %>%
    tab_style(
        cell_borders(color = gos_blue), 
        cells_body()
    ) %>%
    opt_table_font(
        font = "Futura"
    )
```

Given the ratio in means; I would recommend using the `1 / bin_vars` as the weighting rather than the linear approximation; I trust the LOOCV evaluation over the test MSE evaluation.

```{r}
mean(bin_weight_loocvs - raw_loocvs) / 
    mean(linear_weight_loocvs - raw_loocvs)
```

How many events saw improvement in the OPR fit from weighting? 

```{r}
sum((raw_loocvs / bin_weight_loocvs) > 1) / nrow(viz)
```

What percent better are the weighted values on average?

```{r}
mean(((raw_loocvs / bin_weight_loocvs) - 1) * 100)
```
