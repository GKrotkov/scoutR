---
title: "OPR Weighting Evaluation"
format: html
---

```{r setup, message = FALSE, warning = FALSE}
library(devtools)
load_all()
rm(list = ls())
load("../../data/district_quals_09_24.rda")

# normalize match scores so we can do cross-week and cross-season comparisons
# this simplifies a lot of later code
matches <- lapply(
    matches, 
    function(match_df){
        scaled <- scale(c(match_df$red_score, match_df$blue_score))
        match_df$red_score <- scaled[1:nrow(match_df)]
        match_df$blue_score <- scaled[(nrow(match_df) + 1):length(scaled)]
        return(match_df)
    })

library(extrafont)
library(gt)
gos_blue <- "#337DFC"
gos_red <- "#F7041A"

gos_theme <- theme_bw() + 
    theme(text = element_text(family = "Futura"))

load("../../data/opr_weights.rda")
load("../../data/optimized_opr_weighting.rda")

weighted_fit <- function(match_df, w){
    return(
        fit_lineup_lm(
            match_df, 
            responses = list(
                red = match_df$red_score, blue = match_df$blue_score
            ), 
            w = w
        )
    )
}
```

# Test MSE

Reasons to reserve the last 3/12 matches instead of randomly selecting: 
- Casts the problem as a prediction, rather than inference, problem: we are interested in approximating a team's performance heading into the playoff rounds 
- Follows the sequential nature of the data 
- Treats every event in the same manner, while if we randomly picked there would be an interaction with the year effect which would be difficult to account for

```{r optimization-helpers}
# assume matches is a dataframe of matches with red_score and blue_score columns
# uses WLS for computational efficiency
# computes the ratio between the unweighted and weighted model test MSE
compute_mse_ratio <- function(matches, w){
    # reserve the last 3/12 matches for testing
    test <- weight_rows(matches, c(rep(0, 3), 1))
    train <- weight_rows(matches, c(rep(1, 3), 0))

    null_fit <- fit_lineup_lm(
        train, list(red = train$red_score, blue = train$blue_score)
    )

    weighted_fit <- fit_lineup_lm(
        train, list(red = train$red_score, blue = train$blue_score), w = w
    )

    test_design <- lineup_design_matrix(test)
    test_design$response <- c(test$blue_score, test$red_score)
    
    preds_null <- predict(null_fit, newdata = test_design)
    preds_weighted <- predict(weighted_fit, newdata = test_design)
    true_response <- c(test$blue_score, test$red_score)
    
    mse_null <- mean((preds_null - true_response) ^ 2)
    mse_weighted <- mean((preds_weighted - true_response) ^ 2)
    return(mse_null / mse_weighted)
}

# matches - list of dataframes, each with qual matches with >= 12 matches/team
# w - vector representing the weighting
# returns a vector of mean squared errors for each event in `matches`, where
# negative means that the weighted models performed better and positive means
# that the null models performed better.
test_weighting <- function(matches, w){
    # don't need to normalize weights anymore because the helper function does
    diffs <- sapply(
        matches, compute_mse_ratio, w = w
    )
    names(diffs) <- names(matches)
    return(diffs)
}
```

Use the bootstrap to approximate a confidence interval for the ratio (raw OPR MSE / weighted OPR MSE)

```{r test-mse-bootstrap}
mse_ratios <- test_weighting(matches, w_stepwise)

viz <- data.frame(mse_ratio = mse_ratios, 
                  year = as.factor(substr(names(mse_ratios), 1, 4)))

# generate B bootstrap samples and compute the mean MSE ratio for each sample.
B <- 1000
bstrap_ratios <- rep(NA, B)
for (i in 1:B){
    bstrap_sample <- sample(mse_ratios, length(mse_ratios), replace = TRUE)
    bstrap_ratios[i] <- mean(bstrap_sample)
}
# Now we can take the 2.5th percentile value and the 97.5th percentile value 
# from those ratios to define the lower and upper bound for a 95% CI
bstrap_ratios <- sort(bstrap_ratios)
# lower bound for mean test MSE ratio
bstrap_lower <- bstrap_ratios[floor(B * 0.025)]
# upper bound for mean test MSE ratio
bstrap_upper <- bstrap_ratios[ceiling(B * 0.975)]
```

```{r test-mse-visualization}
ggplot(viz, aes(x = mse_ratio)) + 
    geom_histogram(fill = gos_blue, color = "black", 
                   bins = ceiling(sqrt(nrow(viz)))) + 
    geom_vline(aes(xintercept = mean(mse_ratio)), color = gos_red) + 
    geom_vline(aes(xintercept = bstrap_lower), color = gos_red, lty = 3) + 
    geom_vline(aes(xintercept = bstrap_upper), color = gos_red, lty = 3) + 
    annotate("label", x = mean(mse_ratios), y = 15, 
             label = paste0("Mean: ", round(mean(mse_ratios), 3), "\n", 
                            "95% CI: (", round(bstrap_lower, 3), ", ", 
                            round(bstrap_upper, 3), ")")) +
    labs(title = "OPR Weighting Test MSE Ratio (Stepwise Optimized)", 
         subtitle = "District Event Qualifications, 2009 - 2024", 
         x = "Event-Normalized MSE Ratio (null:weighted)", 
         y = "# Events") +
    gos_theme

ggplot(viz, aes(x = mse_ratio)) + 
    geom_density(alpha = 0.2, fill = gos_blue) + 
    facet_wrap(~year) +
    labs(title = "OPR Weighting Test MSE Ratio", x = "Test MSE Ratio", 
         y = "Density") + 
    gos_theme

viz <- viz %>%
    group_by(year) %>%
    summarize(events = n(), 
              mean_diff = mean(mse_ratio), 
              median_diff = median(mse_ratio))

colnames(viz) <- c(
    "Year", "# Events", "Mean MSE Ratio", "Median MSE Ratio"
)

gt(viz) %>%
    tab_header(
        title = "Yearly MSE ratios raw:weighted OPR"
    ) %>%
    tab_options(
        column_labels.background.color = gos_blue
    ) %>%
    tab_style(
        cell_borders(color = gos_blue), 
        cells_body()
    ) %>%
    opt_table_font(
        font = "Futura"
    )
```

# In-Sample MSE

```{r in-sample-mse}
compute_in_sample_mse <- function(matches, w){
    fits <- lapply(matches, weighted_fit, w = w)
    mses <- sapply(
        fits, function(fit){
           return(mean(residuals(fit) ^ 2))
        })
    return(mses)
}

stepwise_weighted_mse <- compute_in_sample_mse(matches, w_stepwise)
fitvar_weighted_mse <- compute_in_sample_mse(matches, w_bin_fitted_vars)
residvar_weighted_mse <- compute_in_sample_mse(matches, w_bin_resid_vars)
raw_mse <- compute_in_sample_mse(matches, 1)

viz <- data.frame(stepwise_ratio = raw_mse / stepwise_weighted_mse, 
                  fitvar_ratio = raw_mse / fitvar_weighted_mse, 
                  residvar_ratio = raw_mse / residvar_weighted_mse,
                  year = as.factor(substr(names(raw_mse), 1, 4)))

ggplot(viz, aes(x = stepwise_ratio)) + 
    geom_histogram(fill = gos_blue, color = "black", 
                   bins = ceiling(sqrt(nrow(viz)))) + 
    geom_vline(aes(xintercept = mean(stepwise_ratio)), color = gos_red) + 
    annotate("label", x = mean(viz$stepwise_ratio), y = 15, 
             label = paste0("Mean: ", round(mean(viz$stepwise_ratio), 6))) +
    labs(title = "In-Sample MSE Ratio (raw / weighted by stepwise optimization)", 
         subtitle = "District Event Qualifications, 2009 - 2024", 
         x = "Event-Normalized Mean Squared Error Difference (Std Devs)", 
         y = "# Events") +
    gos_theme

ggplot(viz, aes(x = fitvar_ratio)) + 
    geom_density(alpha = 0.2, fill = gos_blue) + 
    facet_wrap(~year) +
    labs(title = "OPR Weighting MSE Ratio", x = "MSE Ratio", 
         y = "Density") + 
    gos_theme

viz <- viz %>%
    group_by(year) %>%
    summarize(events = n(), 
              mean_diff = mean(fitvar_ratio), 
              median_diff = median(fitvar_ratio))

colnames(viz) <- c(
    "Year", "# Events", "Mean MSE Ratio", "Median MSE Ratio"
)

gt(viz) %>%
    tab_header(
        title = "Yearly MSE ratios between null and weighted OPR"
    ) %>%
    tab_options(
        column_labels.background.color = gos_blue
    ) %>%
    tab_style(
        cell_borders(color = gos_blue), 
        cells_body()
    ) %>%
    opt_table_font(
        font = "Futura"
    )
```

# LOOCV

Advantages of LOOCV: 
- Retains more of the training test so the weighting makes more sense
- Unbiased estimator of prediction risk, low variance because of the averaging over the entire hat matrix.
- Shortcut formula can make it computationally efficient
    - Makes weight optimization over a grid possible

```{r loocv-compute}
# this uses the "shortcut" formula for LOOCV
# see formula 5.2 in ISLR by James, Witten, Hastie, and Tibshirani (v2), pg 202
LOOCV <- function(fit){
    mean(((fit$model$response - predict(fit))/(1 - boot::glm.diag(fit)$h))^2)
}

linear_weight_fits <- lapply(matches, weighted_fit, w = w_linear)
bin_weight_fits <- lapply(matches, weighted_fit, w = w_bin_resid_vars)
fitted_var_weight_fits <- lapply(matches, weighted_fit, w = w_bin_fitted_vars)
stepwise_fits <- lapply(matches, weighted_fit, w = w_stepwise)
raw_fits <- lapply(matches, weighted_fit, w = 1)

linear_weight_loocvs <- sapply(linear_weight_fits, LOOCV)
resid_weight_loocvs <- sapply(bin_weight_fits, LOOCV)
fitted_weight_loocvs <- sapply(fitted_var_weight_fits, LOOCV)
stepwise_loocvs <- sapply(stepwise_fits, LOOCV)
raw_loocvs <- sapply(raw_fits, LOOCV)
```

Use the bootstrap to approximate the confidence interval on the ratio of LOOCV 

@TODO bootstrap as with test MSE; probably also need to clean up the visualization code to show the stepwise-optimized values.

```{r loocv-bootstrap}
viz <- data.frame(linear_loocv_ratio = raw_loocvs / linear_weight_loocvs, 
                  resid_loocv_ratio = raw_loocvs / resid_weight_loocvs,
                  fitted_loocv_ratio = raw_loocvs / fitted_weight_loocvs,
                  stepwise_loocv_ratio = raw_loocvs / stepwise_loocvs,
                  year = as.factor(substr(names(raw_loocvs), 1, 4)))
```

```{r loocv-viz}
ggplot(viz, aes(x = stepwise_loocv_ratio)) + 
    geom_histogram(fill = gos_blue, color = "black", 
                   bins = ceiling(sqrt(nrow(viz)))) + 
    geom_vline(aes(xintercept = mean(stepwise_loocv_ratio)), color = gos_red) + 
    annotate("label", x = mean(viz$stepwise_loocv_ratio), y = 15, 
             label = paste0("Mean: ", 
                            round(mean(viz$stepwise_loocv_ratio), 3))) +
    labs(title = "OPR Weighting LOOCV Ratio (Stepwise Optimized)", 
         subtitle = "District Event Qualifications, 2009 - 2024", 
         x = "Event-Normalized LOOCV Error Ratio (raw:weighted)", 
         y = "# Events") +
    gos_theme

ggplot(viz, aes(x = stepwise_loocv_ratio)) + 
    geom_density(alpha = 0.2, fill = gos_blue) + 
    facet_wrap(~year) +
    labs(title = "OPR Weighting LOOCV Ratio", x = "LOOCV Difference", 
         y = "Density") + 
    gos_theme

viz <- viz %>%
    group_by(year) %>%
    summarize(events = n(), 
              linear_mean_ratio = mean(linear_loocv_ratio), 
              bin_mean_ratio = mean(resid_loocv_ratio), 
              stepwise_mean_ratio = mean(stepwise_loocv_ratio))

colnames(viz) <- c(
    "Year", "# Events", "Linear", "Binned", "Stepwise"
)

viz$Year <- as.numeric(as.character(viz$Year))
viz <- round(viz, digits = 4)

gt(viz) %>%
    tab_header(
        title = "Mean LOOCV Ratios", 
        subtitle = "unweighted:weighted"
    ) %>%
    tab_options(
        column_labels.background.color = gos_blue
    ) %>%
    tab_style(
        style = cell_text(weight = "bold"), 
        locations = cells_body(columns = "Linear", 
                               rows = which(viz$Linear > viz$Binned))
    ) %>%
    tab_style(
        style = cell_text(weight = "bold"), 
        locations = cells_body(columns = "Binned", 
                               rows = which(viz$Binned >= viz$Linear))
    ) %>%
    tab_style(
        cell_borders(color = gos_blue), 
        cells_body()
    ) %>%
    opt_table_font(
        font = "Futura"
    ) %>%
    tab_footnote(
        footnote = "Higher values indicate the weighting is performing better", 
        locations = cells_title("subtitle")
    )

viz$mean_pct_improvement <- (viz$`Binned` - 1)

ggplot(viz, aes(x = as.character(Year), y = mean_pct_improvement)) + 
    geom_point(aes(size = `# Events`), col = gos_red) + 
    geom_hline(linetype = "dotted", linewidth = 1.25, col = gos_blue, 
               yintercept = mean(viz$mean_pct_improvement)) + 
    geom_hline(yintercept = 0) +
    scale_y_continuous(labels = scales::percent) +
    labs(title = "Weighting Percent Improvement by Year", 
         subtitle = "By Mean LOOCV", 
         x = "Year", y = "Mean LOOCV Percent Improvement") + 
    gos_theme
```

Given the ratio in means; I would recommend using the `1 / bin_vars` as the weighting rather than the linear approximation; I trust the LOOCV evaluation over the test MSE evaluation.

```{r}
mean(resid_weight_loocvs - raw_loocvs) / 
    mean(linear_weight_loocvs - raw_loocvs)
```

How many events saw improvement in the OPR fit from weighting? 

```{r}
sum((raw_loocvs / resid_weight_loocvs) > 1) / length(raw_loocvs)
```

What percent better are the weighted values on average?

```{r}
mean(((raw_loocvs / resid_weight_loocvs) - 1) * 100)
```
