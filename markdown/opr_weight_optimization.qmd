---
title: "opr_weight_optimization"
format: html
---

# Estimating Weights as a function of residuals

It is difficult to optimize the weights because we are optimizing over an uncountably infinite grid. So we are interested in directly estimating the weights rather than numerically optimizing them.

```{r setup, message = FALSE, warning = FALSE}
library(devtools)
load_all()
rm(list = ls())
load("../data/district_quals_09_24.rda")
library(extrafont)
library(plotly)
gos_blue <- "#337DFC"
gos_red <- "#F7041A"
```

To estimate the weighting, we will fit a regular unweighted regression for each event and investigate the relationship between the residuals and the match number. To make comparison possible between events with different numbers of matches, we need to convert each match to a proportion of the progress through the tournament that the match took place at.

```{r}
fit_opr <- function(match_df){
    return(
        fit_lineup_lm(
            match_df, responses = list(
                red = match_df$red_score, blue = match_df$blue_score)
        )
    )
}

normalize_match_number <- function(opr_residual){
    # we know we can divide the length by 2 because there are 2 alliances/match
    alliance_corrected <- ((1:length(opr_residual) - 1) %% 
                               (length(opr_residual) / 2)) + 1
    return((alliance_corrected / length(opr_residual)) * 2)
}

fits <- lapply(matches, fit_opr)

residuals <- lapply(fits, function(fit){return(residuals(fit))})

normalized_resids <- lapply(residuals, function(residuals){
    return(scale(residuals, center = FALSE))
})

viz <- data.frame(
    resids = unlist(residuals, use.names = FALSE),
    std_resids = unlist(normalized_resids, use.names = FALSE),
    sq_resids = unlist(residuals, use.names = FALSE) ^ 2,
    sq_norm_resids = unlist(normalized_resids, use.names = FALSE) ^ 2,
    match_percentile = unlist(lapply(residuals, normalize_match_number), 
                              use.names = FALSE)
)

sq_norm_resid_fit <- lm(sq_norm_resids ~ match_percentile, data = viz)

ggplot(viz, aes(x = match_percentile, y = sq_norm_resids)) + 
    geom_point(col = gos_red, alpha = 0.2) + 
    geom_smooth(method = "lm", formula = y ~ x, col = "black") +
    annotate("label", 
             x = mean(viz$match_percentile), 
             y = mean(viz$sq_norm_resids), 
             label = paste0(
                 "Slope: ", round(coefficients(sq_norm_resid_fit)[2], 3))
             ) + 
    labs(title = "SLR residuals slightly decrease as the tournament proceeds", 
         x = "Qual Match Percentile", y = "Squared Normalized Residual") + 
    theme(text = element_text(family = "Futura"), 
          panel.background = element_rect(fill = "#FFFFFF", color = "#FFFFFF", 
                                          linewidth = 0.5, linetype = "solid"), 
          panel.grid.major = element_line(linewidth = 0.5, linetype = "solid", 
                                          color = gos_blue), 
          panel.grid.minor = element_line(linewidth = 0.25, linetype = "solid", 
                                          color = gos_blue))

log_log_fit <- lm(log(sq_norm_resids) ~ log(match_percentile), data = viz)

ggplot(viz, aes(x = log(match_percentile), y = log(sq_norm_resids))) + 
    geom_point(col = gos_red, alpha = 0.2) + 
    geom_smooth(method = "lm", formula = y ~ x, col = "black") +
    annotate("label", 
             x = mean(log(viz$match_percentile)), 
             y = mean(log(viz$sq_norm_resids)), 
             label = paste0(
                 "Slope: ", round(coefficients(log_log_fit)[2], 3))
             ) + 
    labs(title = "Log-log transformation improves the linearity of the fit", 
         x = "Log Qual Match Percentile", 
         y = "Log Squared Normalized Residual") + 
    theme(text = element_text(family = "Futura"), 
          panel.background = element_rect(fill = "#FFFFFF", color = "#FFFFFF", 
                                          linewidth = 0.5, linetype = "solid"), 
          panel.grid.major = element_line(linewidth = 0.5, linetype = "solid", 
                                          color = gos_blue), 
          panel.grid.minor = element_line(linewidth = 0.25, linetype = "solid", 
                                          color = gos_blue))
```

# Estimating the weights by the slope of the residuals

This estimator did not turn out well - actively worse than unweighteds

```{r, include = FALSE}
estimate_weights <- function(event_matches){
    fit <- fit_lineup_lm(
        event_matches, 
        list(red = event_matches$red_score, blue = event_matches$blue_score)
    )
    
    # sort residuals to be in match order
    resid <- c(rbind(
        residuals(fit)[1:nrow(event_matches)], 
        residuals(fit)[(nrow(event_matches) + 1):(2 * nrow(event_matches))]
    ))
    
    # repeat 1:nrow(matches) twice each for the two alliances in each match
    resid_fit <- lm(residuals(fit) ~ rep(1:nrow(event_matches), each = 2))
    # linear prediction fit for the weights
    w <- coefficients(resid_fit)[1] + 
        (coefficients(resid_fit)[2] * 1:nrow(event_matches))
    
    # normalize to ensure nonnegativity
    w <- (w - min(w)) / (max(w) - min(w))
    return(w)
}
```


# Computing Weights by bin variance

```{r}
n_bins <- 11
cuts <- cut(viz$match_percentile, breaks = n_bins)
labs <- levels(cuts)

bin_vars <- rep(0, length(unique(cuts)))
# use standardized residuals to account for year and event week
for (i in 1:length(unique(cuts))){
    bin_vars[i] <- var(unlist(normalized_resids)[cuts == levels(cuts)[i]])
}

plot(x = 1:length(bin_vars), y = bin_vars, 
     xlab = "Bin Number", ylab = "Normalized Residual Variance", 
     main = "Residual Variance by Binned Tournament Progress")
```

Looking at the plot with varying numbers of bins, we'll model the variance in the residuals as two lines, one for the left-hand half of the data and one for the right-hand half of the data. We'll do two regressions to model this, one for match percentiles less than 0.5 and one for match percentiles greater than or equal to 0.5

```{r}
idx <- 1:(floor(length(bin_vars) / 2))
first_half_fit <- lm(bin_vars[idx] ~ idx)
idx <- (ceiling(length(bin_vars) / 2)):length(bin_vars)
second_half_fit <- lm(bin_vars[idx] ~ idx)

plot(x = 1:length(bin_vars), y = bin_vars, 
     xlab = "Bin Number", ylab = "Normalized Residual Variance", 
     main = "Two lines approximate the variance relationship")
abline(first_half_fit, col = gos_blue)
abline(second_half_fit, col = gos_red)
```

Now we need to write the variances in terms of the slopes, and then turn the variances into weights by inverting them.

```{r}
weights_from_lines <- function(intercept_1, intercept_2, n_bins, slope_1, slope_2){
    first_half <- rep(intercept_1, floor(n_bins / 2))
    first_half <- first_half + (slope_1 * (1:length(first_half) - 1))
    second_half <- rep(intercept_2 + (slope_2 * ((floor(n_bins / 2) + 1))), 
                       ceiling(n_bins / 2))
    second_half <- second_half + (slope_2 * (1:length(second_half) - 1))
    result <- 1 / c(first_half, second_half)
    names(result) <- NULL
    return(result)
}

w <- weights_from_lines(
    coefficients(first_half_fit)[1], coefficients(second_half_fit)[1], n_bins,
    coefficients(first_half_fit)[2], coefficients(second_half_fit)[2]
)
```

# Testing Weight Optimization

Reasons to reserve the last 2/12 matches instead of randomly selecting: 
- Casts the problem as a prediction, rather than inference, problem: we are interested in approximating a team's performance heading into the playoff rounds
- Follows the sequential nature of the data
- Treats every event in the same manner, while if we randomly picked there would be an interaction with the year effect which would be difficult to account for

```{r optimization-helpers}
# redefine compute_mse_diff to use WLS
# assume matches is a dataframe of matches with red_score and blue_score columns
compute_mse_diff <- function(matches, w){
    # reserve the last 2/12 matches for testing
    test <- weight_rows(matches, c(rep(0, 5), 1))
    train <- weight_rows(matches, c(rep(1, 5), 0))

    null_fit <- fit_lineup_lm(
        train, list(red = train$red_score, blue = train$blue_score)
    )

    weighted_fit <- fit_lineup_lm(
        train, list(red = train$red_score, blue = train$blue_score), w = w
    )

    test_design <- lineup_design_matrix(test)
    test_design$response <- c(test$blue_score, test$red_score)
    
    preds_null <- predict(null_fit, newdata = test_design)
    preds_weighted <- predict(weighted_fit, newdata = test_design)
    true_response <- c(test$blue_score, test$red_score)
    
    mse_null <- mean((preds_null - true_response) ^ 2)
    mse_weighted <- mean((preds_weighted - true_response) ^ 2)
    return(mse_weighted - mse_null)
}

# matches - list of dataframes, each with qual matches with >= 12 matches/team
# w - vector representing the weighting
# returns a vector of mean squared errors for each event in `matches`, where
# negative means that the weighted models performed better and positive means
# that the null models performed better.
test_weighting <- function(matches, w){
    # don't need to normalize weights anymore because the helper function does
    diffs <- sapply(
        matches, compute_mse_diff, w = w
    )
    names(diffs) <- names(matches)
    
    # normalize errors for cross-year comparisons
    unique_years <- unique(as.numeric(substr(names(matches), 1, 4)))
    result <- unlist(lapply(
        unique_years, 
        function(year, diffs){
            batch <- diffs[startsWith(names(diffs), as.character(year))]
            scale(batch, center = FALSE)
        }, 
        diffs
    ))
    names(result) <- names(diffs)
    
    return(result)
}
```

```{r visualization}
library(extrafont)
library(gt)

gos_blue <- "#337DFC"
gos_red <- "#F7041A"

gos_theme <- theme_bw() + 
    theme(text = element_text(family = "Futura"))

mse_diffs <- test_weighting(matches, w)

viz <- data.frame(mse_diff = mse_diffs, 
                  year = as.factor(substr(names(mse_diffs), 1, 4)))

ggplot(viz, aes(x = mse_diff)) + 
    geom_histogram(fill = gos_blue, color = "black", 
                   bins = ceiling(sqrt(nrow(viz)))) + 
    geom_vline(aes(xintercept = mean(mse_diff)), color = gos_red) + 
    annotate("label", x = mean(mse_diffs), y = 15, 
             label = paste0("Mean: ", round(mean(mse_diffs), 3))) +
    labs(title = "Weighting Improves OPR Predictions", 
         subtitle = "District Event Qualifications, 2009 - 2024", 
         x = "Year-Normalized Mean Squared Error Difference (Std Devs)", 
         y = "# Events") +
    gos_theme

ggplot(viz, aes(x = mse_diff)) + 
    geom_density(alpha = 0.2, fill = gos_blue) + 
    facet_wrap(~year) +
    labs(title = "OPR Weighting MSE Difference", x = "MSE Difference", 
         y = "Density") + 
    gos_theme

viz <- viz %>%
    group_by(year) %>%
    summarize(events = n(), 
              mean_diff = mean(mse_diff), 
              median_diff = median(mse_diff))

colnames(viz) <- c(
    "Year", "# Events", "Mean MSE Difference", "Median MSE Difference"
)

gt(viz) %>%
    tab_header(
        title = "Yearly MSE differences between null and weighted OPR"
    ) %>%
    tab_options(
        column_labels.background.color = gos_blue
    ) %>%
    tab_style(
        cell_borders(color = gos_blue), 
        cells_body()
    ) %>%
    opt_table_font(
        font = "Futura"
    )
```
