---
title: "OPR Weighting"
format: pdf
editor: visual
---

```{r libraries}
library(devtools)
load_all()
```

# Gentle OPR Weighting does improve the R\^2 slightly for 2022TXIRV

```{r}
library(devtools)
load_all()

event_key <- "2022txirv"

matches <- event_matches(event_key, match_type = "qual")
# order matches before computing design matrix and response
matches <- matches[order(matches$match_number), ]

# apply weighting before computation
# assume the number of weights evenly divides the number of matches
# assume all weights are integers (this assumption could be relaxed)
weights <- c(3, 4, 4, 5)
cuts <- cut(1:nrow(matches), length(weights))

result <- data.frame()
for (i in seq_along(weights)){
    bin <- matches[cuts == cuts[i], ]
    result <- rbind(result, bin[rep(seq_len(nrow(bin)), weights[i]), ])
}

fit <- fit_lineup_lm(
    result, list(red = result$red_score, blue = result$blue_score)
)

null_fit <- fit_lineup_lm(
    matches, list(red = matches$red_score, blue = matches$blue_score)
)

cat("\n", "Null fit adj. R^2:", summary(null_fit)$adj.r.squared, "\n", 
    "Weighted fit adj. R^2:", summary(fit)$adj.r.squared, "\n", 
    "diff:", summary(fit)$adj.r.squared - summary(null_fit)$adj.r.squared, "\n")
```

# Proving the helper functions work

```{r}
library(devtools)
load_all()

event_key <- "2024paca"

matches <- event_matches(event_key, match_type = "qual")
# order matches before computing design matrix and response
matches <- matches[order(matches$match_number), ]

weights <- c(rep(0, 25), rep(1, 60), rep(25, 24), rep(50, 1))

weighted <- weight_rows(matches, weights)

weighted_fit <- fit_lineup_lm(
    weighted, list(red = weighted$red_score, blue = weighted$blue_score)
)

null_fit <- fit_lineup_lm(
    matches, list(red = matches$red_score, blue = matches$blue_score)
)

cat("\n", "Null fit adj. R^2:", summary(null_fit)$adj.r.squared, "\n", 
    "Weighted fit adj. R^2:", summary(weighted_fit)$adj.r.squared, "\n", 
    "diff:", 
    summary(weighted_fit)$adj.r.squared - summary(null_fit)$adj.r.squared, "\n")
```

# Testing whether throwing out the first two rounds of matches improves OPR

I'll restrict this code to districts because always having 12 qualification matches makes things significantly easier.

```{r}
library(devtools)
load_all()

event_key <- "2023txfor"

matches <- event_matches(event_key, match_type = "qual")
matches <- matches[order(matches$match_number), ]

roundlen <- nrow(matches) / 12

# reserve the last 2 matches for testing
test <- weight_rows(matches, c(rep(0, 10), rep(1, 2)))
train <- weight_rows(matches, c(rep(1, 10), rep(0, 2)))

null_fit <- fit_lineup_lm(
    train, list(red = train$red_score, blue = train$blue_score)
)

# throw out the first 2 rounds of matches
weighted <- weight_rows(
    train, c(3, 4, 4, 4, 5)
)

weighted_fit <- fit_lineup_lm(
    weighted, list(red = weighted$red_score, blue = weighted$blue_score)
)

test_design <- lineup_design_matrix(test)
test_design$response <- c(test$blue_score, test$red_score)

preds_null <- predict(null_fit, newdata = test_design)
preds_weighted <- predict(weighted_fit, newdata = test_design)
true_response <- c(test$blue_score, test$red_score)

mse_null <- mean((preds_null - true_response) ^ 2)
mse_weighted <- mean((preds_weighted - true_response) ^ 2)

cat("\n", "Null MSE:", mse_null, "\n", 
    "Weighted MSE:", mse_weighted, "\n")
```

Testing for all districts in 2022

Picking 2022 because that was a generally excellent year for OPR (scoring highly separable and linear)

```{r}
# assume "w" is a length-5 vector where each index represents an integer
# weight on the importance of a set of 2 "rounds" of matches (each team playing)
# 2 matches, on average
# assume "matches" is a dataframe of qualification matches, *in order*, of
# a district competition (or regional with 12 quals)
compute_mse_diff <- function(matches, w){
    # assumption: we are looking at districts only
    roundlen <- nrow(matches) / 12

    # reserve the last 2 matches for testing
    test <- weight_rows(matches, c(rep(0, 10), rep(1, 2)))
    train <- weight_rows(matches, c(rep(1, 10), rep(0, 2)))

    null_fit <- fit_lineup_lm(
        train, list(red = train$red_score, blue = train$blue_score)
    )

    # apply given weighting
    weighted <- weight_rows(train, w)

    weighted_fit <- fit_lineup_lm(
        weighted, list(red = weighted$red_score, blue = weighted$blue_score)
    )

    test_design <- lineup_design_matrix(test)
    test_design$response <- c(test$blue_score, test$red_score)
    
    preds_null <- predict(null_fit, newdata = test_design)
    preds_weighted <- predict(weighted_fit, newdata = test_design)
    true_response <- c(test$blue_score, test$red_score)
    
    mse_null <- mean((preds_null - true_response) ^ 2)
    mse_weighted <- mean((preds_weighted - true_response) ^ 2)
    return(mse_weighted - mse_null)
}
```

```{r}
district_keys <- districts(2022)$key

# remove 2022 ONT and CHS bc they did single-day events
district_keys <- setdiff(district_keys, c("2022ont", "2022chs"))
event_keys <- unlist(sapply(district_keys, district_events, keys = TRUE))
# remove CMP-level events (ONT, MI, TX, and NE have divisions)
event_keys <- setdiff(
    event_keys, event_keys[grep("(mi|ont|tx|ne)cmp$", event_keys)]
)
# 2023 Granite State didn't play the 2nd half of quals
event_keys <- setdiff(event_keys, "2023nhgrs")

matches <- lapply(event_keys, event_matches, match_type = "qual")
# order matches by match number
matches <- lapply(
    matches, function(matches){matches[order(matches$match_number), ]}
)

diffs <- sapply(
    matches, compute_mse_diff, 
    w = c(3, 4, 4, 4, 5)
)

hist(diffs)
abline(v = mean(diffs), col = "red")

```

# Testing a single weighting over all district events 2009 - 2024

```{r}
load("../data/district_quals_09_24.rda")

weights <- c(0.75, 1, 1, 1, 1.25)
diffs <- sapply(
    matches, compute_mse_diff, w = normalize_weights(weights)
)
names(diffs) <- names(matches)

# normalize errors for cross-year comparisons
result <- unlist(lapply(
    setdiff(2009:2024, c(2020, 2021)), 
    function(year, diffs) {
        batch <- diffs[startsWith(names(diffs), as.character(year))]
        scale(batch, center = FALSE)}, 
    diffs
))
names(result) <- names(diffs)

viz <- result[grep("^2018", names(result))]

# 2018 was bad for weighting, but all other years have a negative mean
hist(viz, main = "Impact of weighting in 2018")
abline(v = mean(viz), col = "red")

viz <- result

hist(viz, main = "Impact of weighting from 2009 - 2024")
abline(v = mean(viz), col = "red")
```

# Demonstrate that WLS is essentially identical to row-weighting

```{r}
load("../data/district_quals_09_24.rda")

evnt <- matches[["2022txirv"]]

design_unweighted <- lineup_design_matrix(evnt)
design_unweighted$response <- c(evnt$blue_score, evnt$red_score)

w <- c(rep(0.9, nrow(evnt) / 2), rep(1.1, nrow(evnt) / 2))

# apply given weighting
matches_weighted <- weight_rows(evnt, w)
design_weighted <- lineup_design_matrix(matches_weighted)
design_weighted$response <- c(matches_weighted$blue_score, 
                              matches_weighted$red_score)

null_fit <- lm(response ~ 0 + ., data = design_unweighted)
weighted_fit <- lm(response ~ 0 + ., data = design_weighted)
wls_fit <- lm(response ~ 0 + ., data = design_unweighted, weights = rep(w, 2))

# check that they are within a tolerance of e-10
all(round(coefficients(weighted_fit) - coefficients(wls_fit), digits = 8) == 0)
```

# Fit weights from OLS residuals

```{r}
rm(list = ls())
load("../data/district_quals_09_24.rda")

# redefine compute_mse_diff to use WLS
compute_mse_diff <- function(matches, w){
    # reserve the last 2 matches for testing
    test <- weight_rows(matches, c(rep(0, 10), rep(1, 2)))
    train <- weight_rows(matches, c(rep(1, 10), rep(0, 2)))

    null_fit <- fit_lineup_lm(
        train, list(red = train$red_score, blue = train$blue_score)
    )

    weighted_fit <- fit_lineup_lm(
        train, list(red = train$red_score, blue = train$blue_score), w
    )

    test_design <- lineup_design_matrix(test)
    test_design$response <- c(test$blue_score, test$red_score)
    
    preds_null <- predict(null_fit, newdata = test_design)
    preds_weighted <- predict(weighted_fit, newdata = test_design)
    true_response <- c(test$blue_score, test$red_score)
    
    mse_null <- mean((preds_null - true_response) ^ 2)
    mse_weighted <- mean((preds_weighted - true_response) ^ 2)
    return(mse_weighted - mse_null)
}

# matches - list of dataframes, each with qual matches with >= 12 matches/team
# w - vector representing the weighting
# returns a vector of mean squared errors for each event in `matches`, where
# negative means that the weighted models performed better and positive means
# that the null models performed better.
test_weighting <- function(matches, w){
    # don't need to normalize weights anymore because the helper function does
    diffs <- sapply(
        matches, compute_mse_diff, w = w
    )
    names(diffs) <- names(matches)
    
    # normalize errors for cross-year comparisons
    unique_years <- unique(as.numeric(substr(names(matches), 1, 4)))
    result <- unlist(lapply(
        unique_years, 
        function(year, diffs){
            batch <- diffs[startsWith(names(diffs), as.character(year))]
            scale(batch, center = FALSE)
        }, 
        diffs
    ))
    names(result) <- names(diffs)
    
    return(result)
}

# @TODO this is a bad estimator
estimate_weights <- function(event_matches){
    fit <- fit_lineup_lm(
        event_matches, 
        list(red = event_matches$red_score, blue = event_matches$blue_score)
    )
    
    # sort residuals to be in match order
    resid <- c(rbind(
        residuals(fit)[1:nrow(event_matches)], 
        residuals(fit)[(nrow(event_matches) + 1):(2 * nrow(event_matches))]
    ))
    
    # repeat 1:nrow(matches) twice each for the two alliances in each match
    resid_fit <- lm(residuals(fit) ~ rep(1:nrow(event_matches), each = 2))
    # linear prediction fit for the weights
    w <- coefficients(resid_fit)[1] + 
        (coefficients(resid_fit)[2] * 1:nrow(event_matches))
    
    # normalize to ensure nonnegativity
    w <- (w - min(w)) / (max(w) - min(w))
    return(w)
}

# rough optimization over a grid found 0.9, 0.9, 1.05, 1.1, 1.1 to be optimal
best_w <- c(0.9, 0.9, 1.05, 1.1, 1.1)
estimated_w <- estimate_weights(matches[["2022txirv"]])
results <- test_weighting(matches, best_w)
# results <- test_weighting(matches, estimated_w)
```

Visualizing the results

```{r}
library(extrafont)
library(gt)

gos_blue <- "#337DFC"
gos_red <- "#F7041A"

viz <- results

par(family = "Futura")
hist(viz, breaks = 25, 
     main = "Weighting Improves OPR Predictions", 
     sub = "District Event Qualifications, 2009 - 2024", 
     xlab = "Year-Normalized Mean Squared Error Difference", ylab = "# Events", 
     col = gos_blue, border = "black")
abline(v = mean(viz), col = gos_red, lwd = 2)

viz <- data.frame(mse_diff = viz, year = as.factor(substr(names(viz), 1, 4)))

ggplot(viz, aes(x = mse_diff)) + 
    geom_density(alpha = 0.2, fill = gos_blue) + 
    facet_wrap(~year) +
    labs(title = "OPR Weighting MSE Difference", x = "MSE Difference")

viz <- viz %>%
    group_by(year) %>%
    summarize(mean_diff = mean(mse_diff), 
              median_diff = median(mse_diff))

colnames(viz) <- c("Year", "Mean MSE Difference", "Median MSE Difference")

gt(viz) %>%
    tab_header(
        title = "By-year MSE differences between weighted OPR and null OPR"
    ) %>%
    tab_options(
        column_labels.background.color = gos_blue
    ) %>%
    tab_style(
        cell_borders(color = gos_blue), 
        cells_body()
    ) %>%
    opt_table_font(
        font = "Futura"
    )
```

# Estimating weights as a function of the residuals

```{r}
load("../data/district_quals_09_24.rda")
matches <- matches[["2022txirv"]]


test_weighting(matches, normalize_weights(estimate_weights(matches)))
```

@TODO test finding weights by using residuals of null model

@TODO how much does the optimal weighting change on a per-game basis? (Prior: not much)

@TODO write a bunch of test cases

@TODO for the kids: replicate these results but in python.

Outstanding questions: 1) Try a more fine-grain weight grid 2) Failure analysis - what happened at the worst events? - Take a look when the bins = 30: What's going on with that?
