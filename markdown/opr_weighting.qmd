---
title: "OPR Weighting"
format: pdf
editor: visual
---

```{r libraries}
library(devtools)
load_all()
```

# Gentle OPR Weighting does improve the R\^2 slightly for 2022TXIRV

```{r}
library(devtools)
load_all()

event_key <- "2022txirv"

matches <- event_matches(event_key, match_type = "qual")
# order matches before computing design matrix and response
matches <- matches[order(matches$match_number), ]

# apply weighting before computation
# assume the number of weights evenly divides the number of matches
# assume all weights are integers (this assumption could be relaxed)
weights <- c(3, 4, 4, 5)
cuts <- cut(1:nrow(matches), length(weights))

result <- data.frame()
for (i in seq_along(weights)){
    bin <- matches[cuts == cuts[i], ]
    result <- rbind(result, bin[rep(seq_len(nrow(bin)), weights[i]), ])
}

fit <- fit_lineup_lm(
    result, list(red = result$red_score, blue = result$blue_score)
)

null_fit <- fit_lineup_lm(
    matches, list(red = matches$red_score, blue = matches$blue_score)
)

cat("\n", "Null fit adj. R^2:", summary(null_fit)$adj.r.squared, "\n", 
    "Weighted fit adj. R^2:", summary(fit)$adj.r.squared, "\n", 
    "diff:", summary(fit)$adj.r.squared - summary(null_fit)$adj.r.squared, "\n")
```

# Proving the helper functions work

```{r}
library(devtools)
load_all()

event_key <- "2024paca"

matches <- event_matches(event_key, match_type = "qual")
# order matches before computing design matrix and response
matches <- matches[order(matches$match_number), ]

weights <- c(rep(0, 25), rep(1, 60), rep(25, 24), rep(50, 1))

weighted <- weight_rows(matches, weights)

weighted_fit <- fit_lineup_lm(
    weighted, list(red = weighted$red_score, blue = weighted$blue_score)
)

null_fit <- fit_lineup_lm(
    matches, list(red = matches$red_score, blue = matches$blue_score)
)

cat("\n", "Null fit adj. R^2:", summary(null_fit)$adj.r.squared, "\n", 
    "Weighted fit adj. R^2:", summary(weighted_fit)$adj.r.squared, "\n", 
    "diff:", 
    summary(weighted_fit)$adj.r.squared - summary(null_fit)$adj.r.squared, "\n")
```

# Testing whether throwing out the first two rounds of matches improves OPR

I'll restrict this code to districts because always having 12 qualification matches makes things significantly easier.

```{r}
library(devtools)
load_all()

event_key <- "2023nhgrs"

matches <- event_matches(event_key, match_type = "qual")
matches <- matches[order(matches$match_number), ]

roundlen <- nrow(matches) / 12

# reserve the last 2 matches for testing
test <- weight_rows(matches, c(rep(0, 10), rep(1, 2)))
train <- weight_rows(matches, c(rep(1, 10), rep(0, 2)))

null_fit <- fit_lineup_lm(
    train, list(red = train$red_score, blue = train$blue_score)
)

# throw out the first 2 rounds of matches
weighted <- weight_rows(
    train, c(3, 4, 4, 4, 5)
)

weighted_fit <- fit_lineup_lm(
    weighted, list(red = weighted$red_score, blue = weighted$blue_score)
)

test_design <- lineup_design_matrix(test)
test_design$response <- c(test$blue_score, test$red_score)

preds_null <- predict(null_fit, newdata = test_design)
preds_weighted <- predict(weighted_fit, newdata = test_design)
true_response <- c(test$blue_score, test$red_score)

mse_null <- mean((preds_null - true_response) ^ 2)
mse_weighted <- mean((preds_weighted - true_response) ^ 2)

cat("\n", "Null MSE:", mse_null, "\n", 
    "Weighted MSE:", mse_weighted, "\n")
```

Testing for all districts in 2022

Picking 2022 because that was a generally excellent year for OPR (scoring highly separable and linear)

```{r}
# assume "w" is a length-5 vector where each index represents an integer
# weight on the importance of a set of 2 "rounds" of matches (each team playing)
# 2 matches, on average
# assume "matches" is a dataframe of qualification matches, *in order*, of
# a district competition (or regional with 12 quals)
compute_mse_diff <- function(matches, w){
    # assumption: we are looking at districts only
    roundlen <- nrow(matches) / 12

    # reserve the last 2 matches for testing
    test <- weight_rows(matches, c(rep(0, 10), rep(1, 2)))
    train <- weight_rows(matches, c(rep(1, 10), rep(0, 2)))

    null_fit <- fit_lineup_lm(
        train, list(red = train$red_score, blue = train$blue_score)
    )

    # apply given weighting
    weighted <- weight_rows(train, w)

    weighted_fit <- fit_lineup_lm(
        weighted, list(red = weighted$red_score, blue = weighted$blue_score)
    )

    test_design <- lineup_design_matrix(test)
    test_design$response <- c(test$blue_score, test$red_score)
    
    preds_null <- predict(null_fit, newdata = test_design)
    preds_weighted <- predict(weighted_fit, newdata = test_design)
    true_response <- c(test$blue_score, test$red_score)
    
    mse_null <- mean((preds_null - true_response) ^ 2)
    mse_weighted <- mean((preds_weighted - true_response) ^ 2)
    return(mse_weighted - mse_null)
}
```

```{r}
district_keys <- districts(2022)$key

# remove 2022 ONT and CHS bc they did single-day events
district_keys <- setdiff(district_keys, c("2022ont", "2022chs"))
event_keys <- unlist(sapply(district_keys, district_events, keys = TRUE))
# remove CMP-level events (ONT, MI, TX, and NE have divisions)
event_keys <- setdiff(
    event_keys, event_keys[grep("(mi|ont|tx|ne)cmp$", event_keys)]
)
# 2023 Granite State didn't play the 2nd half of quals
event_keys <- setdiff(event_keys, "2023nhgrs")

matches <- lapply(event_keys, event_matches, match_type = "qual")
# order matches by match number
matches <- lapply(
    matches, function(matches){matches[order(matches$match_number), ]}
)

diffs <- sapply(
    matches, compute_mse_diff, 
    weights = c(3, 4, 4, 4, 5)
)

hist(diffs)
abline(v = mean(diffs), col = "red")

```

# Reading all district quals 2009 - 2024

```{r read-matches}
# check that a matches dataframe has the correct number of matches and doesn't
# have NA values in key places
flag_conformance <- function(matches){
    # check for unposted matches
    if (!all(matches$blue_score >= 0 & matches$red_score >= 0)) return(FALSE)
    n_teams <- length(unique(unlist(
        matches[, c("red1", "red2", "red3", "blue1", "blue2", "blue3")])))
    # check for at least 12 matches/team
    return(nrow(matches) >= n_teams * 2)
}

district_keys <- unlist(sapply(
    setdiff(2009:2024, c(2020, 2021)), function(yr){districts(yr)$key}
))

event_keys <- unlist(sapply(district_keys, district_events, keys = TRUE))

matches <- lapply(event_keys, event_matches, match_type = "qual")
names(matches) <- event_keys

# remove NAs, events with fewer than 12 matches/team, single-day events
matches <- matches[!is.na(matches)]
matches <- matches[sapply(matches, flag_conformance)]

# order matches by match number
matches <- lapply(
    matches, function(matches){matches[order(matches$match_number), ]}
)

save(matches, file = "data/district_quals_09_24.rda")
```

# Testing a single weighting over all district events 2009 - 2024

```{r}
load("../data/district_quals_09_24.rda")

weights <- c(0.75, 1, 1, 1, 1.25)
diffs <- sapply(
    matches, compute_mse_diff, weights = normalize_weights(weights)
)
names(diffs) <- names(matches)

# normalize errors for cross-year comparisons
result <- unlist(lapply(
    setdiff(2009:2024, c(2020, 2021)), 
    function(year, diffs) {
        batch <- diffs[startsWith(names(diffs), as.character(year))]
        scale(batch, center = FALSE)}, 
    diffs
))
names(result) <- names(diffs)

viz <- result[grep("^2018", names(result))]

# 2018 was bad for weighting, but all other years have a negative mean
hist(viz, main = "Impact of weighting in 2018")
abline(v = mean(viz), col = "red")

viz <- result

hist(viz, main = "Impact of weighting from 2009 - 2024")
abline(v = mean(viz), col = "red")
```

# Optimization over a grid of weights

We will make the key assumption that there are no interaction effects and hope we can get away with a stepwise search of the possible space of weights.

```{r}
# matches - list of dataframes, each with qual matches with >= 12 matches/team
# w - vector of length 5 representing the weighting
# returns a vector of mean squared errors for each event in `matches`, where
# negative means that the weighted models performed better and positive means
# that the null models performed better.
test_weighting <- function(matches, w){
    # normalize weights and compute MSE difference between null and weighted
    diffs <- sapply(
        matches, compute_mse_diff, w = normalize_weights(w)
    )
    names(diffs) <- names(matches)
    
    # normalize errors for cross-year comparisons
    unique_years <- unique(as.numeric(substr(names(matches), 1, 4)))
    result <- unlist(lapply(
        unique_years, 
        function(year, diffs){
            batch <- diffs[startsWith(names(diffs), as.character(year))]
            scale(batch, center = FALSE)
        }, 
        diffs
    ))
    names(result) <- names(diffs)
    
    return(result)
}

load("../data/district_quals_09_24.rda")

# avoiding 1 as a weight for any of them because it will result in division by 0
grid <- seq(0.5, 1.5, 0.2)
min_mean_weights <- rep(0, 5)

for (i in 1:5){
    means <- rep(0, length(grid))
    for (j in seq_along(grid)){
        w <- rep(1, 5)
        w[i] <- grid[j]
        means[j] <- mean(test_weighting(matches, w))
    }
    min_mean_weights[i] <- which.min(means)
}

best_w <- grid[min_mean_weights]
results <- test_weighting(matches, best_w)
```

```{r}
library(extrafont)
library(gt)

gos_blue <- "#337DFC"
gos_red <- "#F7041A"

viz <- results

par(family = "Futura")
hist(viz, breaks = 25, 
     main = "Weighting Improves OPR Predictions", 
     sub = "District Event Qualifications, 2009 - 2024", 
     xlab = "Year-Normalized Mean Squared Error Difference", ylab = "# Events", 
     col = gos_blue, border = "black")
abline(v = mean(viz), col = gos_red, lwd = 2)

viz <- data.frame(mse_diff = viz, year = as.factor(substr(names(viz), 1, 4)))

ggplot(viz, aes(x = mse_diff)) + 
    geom_density(alpha = 0.2, fill = gos_blue) + 
    facet_wrap(~year) +
    labs(title = "OPR Weighting MSE Difference", x = "MSE Difference")

viz <- viz %>%
    group_by(year) %>%
    summarize(mean_diff = mean(mse_diff))

colnames(viz) <- c("Year", "MSE Difference")

gt(viz) %>%
    tab_header(
        title = "By-year MSE differences between weighted OPR and null OPR"
    ) %>%
    tab_options(
        column_labels.background.color = gos_blue
    ) %>%
    tab_style(
        cell_borders(color = gos_blue), 
        cells_body()
    ) %>%
    opt_table_font(
        font = "Futura"
    )
```

@TODO for the kids: replicate these results but in python. 
@TODO debug event_robot_results, something wrong with event_matches function

Outstanding questions: 
1) Do the best weightings vary from year to year? 
2) Failure analysis - what happened at the worst events?
    - Take a look when the bins = 30: What's going on with that?
